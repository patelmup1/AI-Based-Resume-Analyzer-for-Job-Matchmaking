# -*- coding: utf-8 -*-
"""Resume Analyzer v2: Enhanced Edition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e35FrKY7s7lCFzbYvDLzviQPDZ_1hWLt
"""

# --- STEP 1: Install Required Libraries ---
!pip install textract==1.6.3
!pip install spacy transformers torch scikit-learn nltk sentence-transformers PyPDF2 matplotlib seaborn python-docx google-colab
!pip install huggingface_hub

# System tools for file parsing
!apt-get install -y poppler-utils
!apt-get install -y antiword

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# --- STEP 2: Import Libraries ---
import os
import spacy
import PyPDF2
import textract
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util
from huggingface_hub import InferenceClient

# Replace 'your_token_here' with your actual HF token
hf_token = "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"
client = InferenceClient(model="mistralai/Mistral-7B-Instruct-v0.2", token=hf_token)

def extract_text_from_resume(file_path):
    if file_path.endswith('.pdf'):
        with open(file_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            return ''.join(page.extract_text() for page in reader.pages)
    elif file_path.endswith(('.doc', '.docx')):
        return textract.process(file_path).decode('utf-8')
    elif file_path.endswith('.txt'):
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        raise ValueError("Unsupported format")

def batch_process_resumes(folder_path, job_description):
    results = []
    for filename in os.listdir(folder_path):
        file_path = os.path.join(folder_path, filename)
        try:
            resume_text = extract_text_from_resume(file_path)
            report = analyze_resume(resume_text, job_description)
            report['filename'] = filename
            report['llm_suggestion'] = get_llm_suggestion(resume_text, job_description)
            results.append(report)
        except Exception as e:
            print(f"Error processing {filename}: {e}")
    return pd.DataFrame(results)

def get_llm_suggestion(resume_text, job_description):
    prompt = f"""
    You are an expert resume advisor.
    A candidate applied for this job: "{job_description}"

    Their resume says: "{resume_text[:2000]}"

    Give one concise suggestion to improve their resume for this job.
    """
    try:
        response = client.text_generation(prompt, max_new_tokens=100, temperature=0.5)
        return response.strip()
    except Exception as e:
        return "Could not generate suggestion."

def save_reports_to_drive(df):
    output_path = "/content/drive/MyDrive/resume_match_report.csv"
    df.to_csv(output_path, index=False)
    print(f"âœ… Reports saved to: {output_path}")

model = SentenceTransformer('all-MiniLM-L6-v2')

SKILLS_DB = ["python", "machine learning", "cloud computing", "sql", "java", "deep learning", "docker", "kubernetes"]

def compute_tfidf_match(resume_text, job_description):
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform([resume_text, job_description])
    return cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0] * 100

def compute_bert_match(resume_text, job_description):
    r_emb = model.encode(resume_text, convert_to_tensor=True)
    j_emb = model.encode(job_description, convert_to_tensor=True)
    return float(util.cos_sim(r_emb, j_emb).item()) * 100

def extract_skills(text):
    return [skill.capitalize() for skill in SKILLS_DB if re.search(rf'\b{skill}\b', text.lower())]

def analyze_resume(resume_text, job_description):
    tfidf_score = compute_tfidf_match(resume_text, job_description)
    bert_score = compute_bert_match(resume_text, job_description)
    overall_score = (tfidf_score + bert_score) / 2

    resume_skills = extract_skills(resume_text)
    job_skills = extract_skills(job_description)

    matched_skills = list(set(resume_skills) & set(job_skills))
    missing_skills = list(set(job_skills) - set(resume_skills))

    # Experience Gap Detection
    exp_pattern = re.compile(r'(experience|worked|developed|engineer|developer).*?(\d+)\s+year', re.IGNORECASE)
    resume_exp = sum(int(m.group(2)) for m in exp_pattern.finditer(resume_text))
    job_exp = sum(int(m.group(2)) for m in exp_pattern.finditer(job_description))
    experience_gap = max(0, job_exp - resume_exp) if job_exp > 0 else 0

    return {
        "overall_score": round(overall_score, 2),
        "keyword_score": round(tfidf_score, 2),
        "semantic_score": round(bert_score, 2),
        "matched_skills": ', '.join(matched_skills),
        "missing_skills": ', '.join(missing_skills),
        "experience_gap": experience_gap,
        "resume_skills": ', '.join(resume_skills),
        "job_skills": ', '.join(job_skills),
    }

def plot_match_scores(df):
    df_sorted = df.sort_values(by='overall_score', ascending=False)
    plt.figure(figsize=(12, 6))
    sns.barplot(x='filename', y='overall_score', data=df_sorted, palette='viridis')
    plt.xticks(rotation=45)
    plt.title("Resume Match Scores")
    plt.ylabel("Match Score (%)")
    plt.xlabel("Resumes")
    plt.tight_layout()
    plt.savefig("/content/drive/MyDrive/resume_match_scores.png")
    plt.show()

# Upload resumes to Colab
from google.colab import files
print("Upload resumes (PDF/DOCX/TXT):")
uploaded = files.upload()

# Save uploaded files to a folder
folder_path = "/content/resumes"
os.makedirs(folder_path, exist_ok=True)
for name in uploaded.keys():
    os.rename(name, os.path.join(folder_path, name))

# Sample job description
job_description = """
We are looking for a Senior Software Engineer with strong experience in Python, Machine Learning, Cloud Computing, and API development.
"""

# Run batch analysis
df = batch_process_resumes(folder_path, job_description)

# Display results
print("\nðŸ“Š Match Results:")
display(df[['filename', 'overall_score', 'missing_skills', 'experience_gap']])

# Plot visualization
plot_match_scores(df)

# Save reports
save_reports_to_drive(df)